---
title: 你们呀，不要想搞个大新闻
date: 2019-11-30
tags: [杂感, 数学]
cover: https://cdn.jsdelivr.net/gh/tansongchen/images@master/big-news.webp
abstract: 可能你已经看出来了，本文标题意在嘲讽近日某科普订阅号的神奇文章。
---

可能你已经看出来了，本文标题意在嘲讽近日某科普订阅号的神奇文章。

## 不明觉厉

前几天刷知乎的时候，看到了这样一则问题：

![网红的线性代数就是不一样](http://img.candobear.com/2019-11-17-230052.png)

根据该文章，数学家陶哲轩对此表示：

> 这个公式看起来好得令人难以置信。
>
> 我完全没想过，子矩阵的特征值编码了原矩阵特征向量的隐藏信息。

是不是觉得特别厉害？我一度以为我在计算物理学课上花两周学到的所有矩阵特征问题算法都可以被扔进历史的垃圾堆了。

## 疑问重重

然而仔细一看，可以看出很多问题。首先是这段莫名其妙的引申：

> 特征向量和特征值的几何本质，其实就是*空间矢量的旋转和缩放*。而中微子的三个味（电子，μ 子，τ 子），不就相当于空间中的三个向量之间的变换吗？

第一句话是对的，矩阵乘法相当于矢量的旋转和缩放，如果矢量恰好是矩阵的特征向量，那么将只有缩放没有旋转，且缩放的倍数就是矩阵的特征值。然而，第二句话就显得牵强了。

我们再来看看在 [arxiv.org](一个在公有区域发表论文的平台。) 上发表的这篇论文本身。

![论文标题：从特征值得到特征向量](http://img.candobear.com/2019-11-17-233157.png)

论文的主要结果是下面的公式：
$$
\left|v_{i, j}\right|^{2} =\frac{\prod_{k=1}^{n-1}\left(\lambda_{i}(A)-\lambda_{k}\left(M_{j}\right)\right)}{\prod_{k=1 ; k \neq i}^{n}\left(\lambda_{i}(A)-\lambda_{k}(A)\right)}
$$
其中，$\lambda_i(A)$ 是矩阵 $A$ 的第 $i$ 个特征值，而 $\lambda_k(M_j)$ 是矩阵 $A$ 删去第 $j$ 行和第 $j$ 列形成的余子阵的第 $k$ 个特征值。根据这些信息，我们可以得到第 $i$ 个特征向量 $v_i$ 的第 $j$ 个元素。

而在现今的常见数值方法中（例如实对称稠密矩阵的 QR 算法），我们是通过将矩阵分解为正交矩阵和上三角矩阵的乘积，然后再将它们反向相乘，如此重复迭代使其收敛于对角矩阵，对角线上即是特征值。这一过程的每一步都是相似变换，如果我们将这些相似变换的矩阵收集起来乘在一起，那么矩阵的特征向量也就能同时得到了。

那么请问，这一公式的应用价值在何处？本来*特征向量可以作为特征值的「副产品」获得*，现在反而要继续计算 $n$ 个子矩阵的特征值，获得的特征向量还存在正负号不定的问题。（对于复数域上的 Hermite 矩阵，就不仅是正负号的问题了，而是一个相位因子 $e^{i\theta}$ 的不确定性。）

## 不要搞个大新闻

从概率的角度讲，线性代数中的特征值问题这类基础算法发生重大改善的几率极小。陶哲轩教授的积极评价，或许是出于有意赞赏，但并不意味着这一结果具有重要价值。所以：

> 你们呀，不要想弄个大新闻，把数学界批判一番。你们呀，naïve！

尽管如此，在某些特殊的情况下，这一结果有可能使计算得以简化。如果你的工作中恰好遇到了这样的问题，欢迎你在留言区与大家分享～



另外，推荐某订阅号尽快使用 Markdown Nice 进行公式排版。这样优雅多了。
